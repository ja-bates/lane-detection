{"cells":[{"cell_type":"markdown","metadata":{"id":"PPD7PSiXcCXf"},"source":["## Import Packages"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":137,"status":"ok","timestamp":1705804713364,"user":{"displayName":"James Bates","userId":"15366358060321474454"},"user_tz":300},"id":"Ms9isbYhcCXf"},"outputs":[],"source":["#importing some useful packages\n","import matplotlib.pyplot as plt\n","import matplotlib.image as mpimg\n","import numpy as np\n","import cv2\n","%matplotlib inline"]},{"cell_type":"markdown","metadata":{"id":"W1XEbBhUcCXg"},"source":["## Read in an Image"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y04GzTygcCXg"},"outputs":[],"source":["#reading in an image\n","image = mpimg.imread('solidYellowLeft.jpg')\n","\n","#printing out some stats and plotting\n","print('This image is:', type(image), 'with dimensions:', image.shape)\n","plt.imshow(image)  # if you wanted to show a single color channel image called 'gray', for example, call as plt.imshow(gray, cmap='gray')"]},{"cell_type":"markdown","metadata":{"id":"S7yEOO8ZcCXh"},"source":["## Helper Functions"]},{"cell_type":"code","execution_count":4,"metadata":{"collapsed":true,"executionInfo":{"elapsed":171,"status":"ok","timestamp":1705804719367,"user":{"displayName":"James Bates","userId":"15366358060321474454"},"user_tz":300},"id":"mLLPx8__cCXh"},"outputs":[],"source":["import math\n","\n","def grayscale(img):\n","    return cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n","    # Or use BGR2GRAY if you read an image with cv2.imread()\n","    # return cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n","\n","def canny(img, low_threshold, high_threshold):\n","    \"\"\"Applies the Canny transform\"\"\"\n","    return cv2.Canny(img, low_threshold, high_threshold)\n","\n","def gaussian_blur(img, kernel_size):\n","    \"\"\"Applies a Gaussian Noise kernel\"\"\"\n","    return cv2.GaussianBlur(img, (kernel_size, kernel_size), 0)\n","\n","def region_of_interest(img, vertices):\n","    \"\"\"\n","    Applies an image mask.\n","\n","    Only keeps the region of the image defined by the polygon\n","    formed from `vertices`. The rest of the image is set to black.\n","    `vertices` should be a numpy array of integer points.\n","    \"\"\"\n","    #defining a blank mask to start with\n","    mask = np.zeros_like(img)\n","\n","    #defining a 3 channel or 1 channel color to fill the mask with depending on the input image\n","    if len(img.shape) > 2:\n","        channel_count = img.shape[2]  # i.e. 3 or 4 depending on your image\n","        ignore_mask_color = (255,) * channel_count\n","    else:\n","        ignore_mask_color = 255\n","\n","    #filling pixels inside the polygon defined by \"vertices\" with the fill color\n","    cv2.fillPoly(mask, vertices, ignore_mask_color)\n","\n","    #returning the image only where mask pixels are nonzero\n","    masked_image = cv2.bitwise_and(img, mask)\n","    return masked_image\n","\n","\n","def draw_lines(img, lines, color=[255, 0, 0], thickness=10):\n","    \"\"\"\n","    This function draws `lines` with `color` and `thickness`.\n","    Lines are drawn on the image inplace (mutates the image).\n","    If you want to make the lines semi-transparent, think about combining\n","    this function with the weighted_img() function below\n","    \"\"\"\n","    for line in lines:\n","        for x1,y1,x2,y2 in line:\n","            cv2.line(img, (x1, y1), (x2, y2), color, thickness)\n","\n","def hough_lines(img, rho, theta, threshold, min_line_len, max_line_gap):\n","    \"\"\"\n","    `img` should be the output of a Canny transform.\n","\n","    Returns an image with hough lines drawn.\n","    \"\"\"\n","    lines = cv2.HoughLinesP(img, rho, theta, threshold, np.array([]), minLineLength=min_line_len, maxLineGap=max_line_gap)\n","    line_img = np.zeros((img.shape[0], img.shape[1], 3), dtype=np.uint8)\n","    draw_lines(line_img, lines)\n","    return line_img, lines\n","\n","# Python 3 has support for cool math symbols.\n","\n","def weighted_img(img, initial_img, α=0.8, β=1., γ=0.):\n","    \"\"\"\n","    `img` is the output of the hough_lines(), An image with lines drawn on it.\n","    Should be a blank image (all black) with lines drawn on it.\n","\n","    `initial_img` should be the image before any processing.\n","\n","    The result image is computed as follows:\n","\n","    initial_img * α + img * β + γ\n","    NOTE: initial_img and img must be the same shape!\n","    \"\"\"\n","    return cv2.addWeighted(initial_img, α, img, β, γ)"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":104,"status":"ok","timestamp":1705702802017,"user":{"displayName":"James Bates","userId":"15366358060321474454"},"user_tz":300},"id":"QDVJCpuMcCXi"},"outputs":[],"source":["import os\n","#os.listdir(\"test_images/\")"]},{"cell_type":"markdown","metadata":{"id":"9Vvd8VS-cCXi"},"source":["## Build a Lane Finding Pipeline\n","\n"]},{"cell_type":"markdown","metadata":{"id":"wDDxEXHK2rLC"},"source":["**Reading Image**"]},{"cell_type":"code","execution_count":5,"metadata":{"collapsed":true,"executionInfo":{"elapsed":119,"status":"ok","timestamp":1705804728537,"user":{"displayName":"James Bates","userId":"15366358060321474454"},"user_tz":300},"id":"MSZhFphtcCXj"},"outputs":[],"source":["from google.colab.patches import cv2_imshow\n","\n","def read_image(img):\n","\n","  #image to hsv\n","  img_hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n","\n","  #useful info\n","  img_height, img_width = img.shape[:2]\n","\n","  return img_hsv, img_width, img_height"]},{"cell_type":"markdown","metadata":{"id":"WZABQXfT2jSq"},"source":["**Colour Thresholding**"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":152,"status":"ok","timestamp":1705804780772,"user":{"displayName":"James Bates","userId":"15366358060321474454"},"user_tz":300},"id":"vtXEoASYhArD"},"outputs":[],"source":["def color_threshold(img_hsv):\n","\n","  #mask of white\n","  mask1 = cv2.inRange(img_hsv, (0,0,230), (255,255,255))\n","  #mask1 = cv2.inRange(img_hsv, (0,0,200), (255,30,255))\n","  #cv2.imwrite(\"isthisworking_white.jpg\", mask1)\n","\n","  #debug\n","  #cv2.imwrite(\"whitefilter2.jpg\", mask1)\n","\n","  #mask of yellow\n","  mask2 = cv2.inRange(img_hsv, (15,50,50), (35, 255, 255))\n","  #mask2 = cv2.inRange(img_hsv, (20,100,100), (40, 255, 255))\n","\n","  #debug\n","  #cv2.imwrite(\"isthisworking_y.jpg\", mask2)\n","\n","  #final mask (white OR yellow)\n","  img_gray = cv2.bitwise_or(mask1, mask2)\n","\n","  #debug\n","  #cv2.imwrite(\"colorthresh.jpg\", img_gray)\n","\n","  return img_gray\n","\n","#could tune this by making a combined binary, threshholding in different color schemes\n","#e.g. https://tjosh.medium.com/finding-lane-lines-with-colour-thresholds-beb542e0d839\n","#set range of values for hue, allow you to catch bigger range of color in different lighting conditions"]},{"cell_type":"markdown","metadata":{"id":"ESzCGlRB2dFv"},"source":["**Smoothing**"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":171,"status":"ok","timestamp":1705804785046,"user":{"displayName":"James Bates","userId":"15366358060321474454"},"user_tz":300},"id":"lwQlUGvFm71r"},"outputs":[],"source":["def smoothing(img_gray):\n","\n","  img_blur = gaussian_blur(img_gray, 5)\n","\n","  #debug\n","  #cv2.imwrite(\"blur.jpg\", img_blur)\n","\n","  return img_blur"]},{"cell_type":"markdown","metadata":{"id":"cL4YTDe72Ctb"},"source":["**Canny Edge Detector**"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":149,"status":"ok","timestamp":1705804788733,"user":{"displayName":"James Bates","userId":"15366358060321474454"},"user_tz":300},"id":"ouNSxKgRm9mB"},"outputs":[],"source":["def edge_detector(img_blur):\n","\n","  img_edges = canny(img_blur, 50, 150)\n","\n","  #debug\n","  #cv2.imwrite(\"canny_50_150.jpg\", img_edges)\n","\n","  return img_edges"]},{"cell_type":"markdown","metadata":{"id":"HszvLCZ81sIY"},"source":["**ROI Selection**"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":129,"status":"ok","timestamp":1705804792342,"user":{"displayName":"James Bates","userId":"15366358060321474454"},"user_tz":300},"id":"c9yI2MATnAyQ"},"outputs":[],"source":["def select_roi(img_edges):\n","\n","  vertices_roi = np.array([[(130, 540), (445, 322), (517, 322), (910, 540)]], dtype=np.int32)\n","\n","  img_roi = region_of_interest(img_edges, vertices_roi)\n","\n","  #debug\n","  #cv2.imwrite(\"roi.jpg\", img_roi)\n","\n","  return img_roi"]},{"cell_type":"markdown","metadata":{"id":"BSO3M90z1U5p"},"source":["**Line Segment Detection (Hough)**"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":183,"status":"ok","timestamp":1705804810784,"user":{"displayName":"James Bates","userId":"15366358060321474454"},"user_tz":300},"id":"k-qC7Eo5nFxv"},"outputs":[],"source":["def line_detection(img_roi):\n","\n","  img_hough, lines = hough_lines(img_roi, 1, np.pi/180, 1, 5, 1)\n","\n","  #debug\n","  #cv2.imwrite(\"hough1_5_1.jpg\", img_hough)\n","\n","  return img_hough, lines"]},{"cell_type":"markdown","metadata":{"id":"oiidE1zq1LRw"},"source":["**Line Segment Classification (Left/Right)**"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":143,"status":"ok","timestamp":1705804821664,"user":{"displayName":"James Bates","userId":"15366358060321474454"},"user_tz":300},"id":"wCJNjjRqnMT0"},"outputs":[],"source":["def line_classification(image, lines, img_width):\n","  left_lines = []\n","  right_lines = []\n","  for line in lines:\n","    for x1,y1,x2,y2 in line:\n","      if x1 < img_width/2:\n","        left_lines.append(line)\n","      else:\n","        right_lines.append(line)\n","  return np.array(left_lines), np.array(right_lines)"]},{"cell_type":"markdown","metadata":{"id":"htGLFG_dcv4n"},"source":["**Line Fitting (RANSAC)**"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":1311,"status":"ok","timestamp":1705804828398,"user":{"displayName":"James Bates","userId":"15366358060321474454"},"user_tz":300},"id":"DIPHGq2SnREG"},"outputs":[],"source":["#ransac fitting: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RANSACRegressor.html\n","from sklearn import linear_model\n","\n","def ransac_line(lines):\n","\n","  #create ransac linear regression instance\n","  ransac = linear_model.RANSACRegressor()\n","\n","  #transpose to get all x's together and y's together\n","  x1 = lines.T[0]\n","  y1 = lines.T[1]\n","  x2 = lines.T[2]\n","  y2 = lines.T[3]\n","\n","  #need to reshape, scikit expects X to be 2 dimensional array\n","  X = np.concatenate((x1[0],x2[0])).reshape(-1,1)\n","\n","  #y is expected to be 1D, don't need to reshape\n","  y = np.concatenate((y1[0],y2[0]))\n","\n","  #ransac algo execution using default variables\n","  ransac.fit(X,y)\n","\n","  inlier_mask = ransac.inlier_mask_\n","  outlier_mask = np.logical_not(inlier_mask)\n","\n","  slope = ransac.estimator_.coef_\n","  yint = ransac.estimator_.intercept_\n","\n","  #debug\n","  #plt.scatter(X, y)\n","  #line_y_values = slope * np.array(X) + yint\n","  #plt.plot(X, line_y_values, color='red', label='Fitted Line')\n","  #print(inlier_mask)\n","  #print(outlier_mask)\n","\n","  return slope, yint\n","\n","\n","def get_points(lines, img):\n","  slope, yint = ransac_line(lines)\n","\n","  #set line to start at bottom of the image (closest to vehicle)\n","  y1 = img.shape[0]\n","  x1 = int((y1 - yint)/slope)\n","\n","  #set line to end 60% height of the image\n","  y2 = int(y1*0.6)\n","  x2 = int((y2 - yint)/slope)\n","\n","  return np.array([x1,y1,x2,y2])\n","\n","def fit_lines(left_lines, right_lines, image):\n","\n","  #Run RANSAC\n","  coordinates_og = np.array([get_points(left_lines, image), get_points(right_lines, image)])\n","\n","  #fix format to comply with draw_lines\n","  coordinates_ok = [[[x1, y1, x2, y2]] for x1, y1, x2, y2 in coordinates_og]\n","\n","  #draw lines\n","  draw_lines(image, coordinates_ok)\n","\n","  return image\n","\n","#debug\n","#plt.imshow(cv2.cvtColor(debug_img_final, cv2.COLOR_BGR2RGB))\n","#plt.show()"]},{"cell_type":"markdown","metadata":{"id":"6EvwfBUTC2w-"},"source":["**Pipeline**"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":305,"status":"ok","timestamp":1705804900123,"user":{"displayName":"James Bates","userId":"15366358060321474454"},"user_tz":300},"id":"oaOMmgQ0C6O5","outputId":"34597552-32e0-4cd1-c34a-86de273dae56"},"outputs":[{"data":{"text/plain":["True"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["#master function\n","def detect_lanes(img):\n","\n","  img_hsv, img_width, img_height = read_image(img)\n","\n","  img_gray = color_threshold(img_hsv)\n","\n","  img_blur = smoothing(img_gray)\n","\n","  img_edges = edge_detector(img_blur)\n","\n","  img_roi = select_roi(img_edges)\n","\n","  img_hough, lines = line_detection(img_roi)\n","\n","  left_lines, right_lines = line_classification(img_hough, lines, img_width)\n","\n","  #debug: create a test image\n","  #debug_img_final = img.copy()\n","\n","  output_image = fit_lines(left_lines, right_lines, img)\n","\n","  return output_image\n","\n","\n","#Running the pipeline\n","\n","img = cv2.imread('solidYellowCurve.jpg')\n","\n","output_image = detect_lanes(img)\n","\n","cv2.imwrite(\"static_output.jpg\", output_image)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"PYZU5v7EcCXj"},"source":["## Test on Videos\n","\n","`solidWhiteRight.mp4`\n","\n","`solidYellowLeft.mp4`\n"]},{"cell_type":"code","execution_count":15,"metadata":{"collapsed":true,"executionInfo":{"elapsed":1161,"status":"ok","timestamp":1705804873101,"user":{"displayName":"James Bates","userId":"15366358060321474454"},"user_tz":300},"id":"pZUO335dcCXj"},"outputs":[],"source":["# Import everything needed to edit/save/watch video clips\n","from moviepy.editor import VideoFileClip\n","from IPython.display import HTML"]},{"cell_type":"code","execution_count":16,"metadata":{"collapsed":true,"executionInfo":{"elapsed":139,"status":"ok","timestamp":1705804874189,"user":{"displayName":"James Bates","userId":"15366358060321474454"},"user_tz":300},"id":"D4VcgQCVcCXj"},"outputs":[],"source":["def process_image(image):\n","\n","    result = detect_lanes(image)\n","    return result"]},{"cell_type":"markdown","metadata":{"id":"y7KM2_FtcCXj"},"source":["Let's try the one with the solid white lane on the right first ..."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SUDDUSqWcCXj"},"outputs":[],"source":["white_output = 'solidWhiteRight_output.mp4'\n","\n","clip1 = VideoFileClip(\"solidWhiteRight.mp4\")\n","\n","white_clip = clip1.fl_image(process_image) #NOTE: this function expects color images!!\n","\n","%time white_clip.write_videofile(white_output, audio=False)\n","#%time clip1.write_videofile(white_output, audio=False)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7d0OzJoIcCXk"},"outputs":[],"source":["yellow_output = 'solidYellowLeft_output.mp4'\n","\n","clip2 = VideoFileClip('solidYellowLeft.mp4')\n","\n","yellow_clip = clip2.fl_image(process_image)\n","\n","%time yellow_clip.write_videofile(yellow_output, audio=False)"]}],"metadata":{"anaconda-cloud":{},"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}
